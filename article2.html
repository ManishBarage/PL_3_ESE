<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Technical Blog</title>
</head>
<body>
    <header>
        <h1>Technical Blog</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="article1.html">Article 1</a></li>
                <li><a href="article2.html">Article 2</a></li>
                <li><a href="article3.html">Article 3</a></li>
                <li><a href="contact.html">Contact Us</a></li>
            </ul>
        </nav>
    </header>

    <section class="main-content">
        <h2>Welcome to our Technical Blog</h2>
        <article>
            <h3>Article 2: The Ray-Ban Meta smart glasses are getting AI-powered visual search features</h3>
            <img src="img3.webp" alt="Article 1 Image">
            <p>The Ray-Ban Meta smart glasses are getting AI-powered visual search features</p>
            <div class="ratings">
                <p>Ratings: ★★★★☆</p>
                <p>Comments: 20</p>
                <p>The updates could go a long way toward making Meta AI feel less gimmicky and more useful, which was one of my top complaints in my initial review of the otherwise impressive smart glasses. Unfortunately, it will likely still be some time before most people with the smart glasses can access the new multimodal functionality. Bosworth said that the early access beta version will only be available in the US to a “small number of people who opt in” initially, with expanded access presumably coming sometime in 2024.

                    Both Mark Zuckerberg shared a few videos of the new capabilities that give an idea of what may be possible. Based on the clips, it appears users will be able to engage the feature with commands that begin with “Hey Meta, look and tell me.” Zuckerberg, for example, asks Meta AI to look at a shirt he’s holding and ask for suggestions on pants that might match. He also shared screenshots showing Meta AI identifying an image of a piece of fruit and translating the text of a meme.
                    
                    In a video posted on Threads, Bosworth said that users would also be able to ask Meta AI about their immediate surroundings as well as more creative questions like writing captions for photos they just shot.
                    
                    This article originally appeared on Engadget at https://www.engadget.com/the-ray-ban-meta-smart-glasses-are-getting-ai-powered-visual-search-features-204556255.html?src=rss
                </p>
            </div>
        </article>
    </section>

    <footer>
        <p>&copy; 2023 Technical Blog. All rights reserved.</p>
    </footer>
</body>
</html>
